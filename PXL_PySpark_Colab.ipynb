{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qV6Grv7qIa9"
      },
      "source": [
        "## Overzicht:\n",
        "<ol>\n",
        "<li>Big Data & PySpark</li>\n",
        "<li>Jupyter Basics</li>\n",
        "<li>Install</li>\n",
        "<li>PySpark Pandas\n",
        "    <ol>\n",
        "    <li>Extract\n",
        "      <ol>\n",
        "        <li>Data inlezen</li>\n",
        "        <li>Data inspecteren</li>\n",
        "        <li>Schema</li>\n",
        "      </ol>\n",
        "    </li>\n",
        "    <li>Transform\n",
        "      <ol>\n",
        "        <li>Select</li>\n",
        "        <li>Insert</li>\n",
        "        <li>Group by</li>\n",
        "        <li>Remove</li>\n",
        "        <li>Update</li>\n",
        "      </ol>\n",
        "    </li>\n",
        "    </ol>\n",
        "</li>\n",
        "<li>Spark SQL</li>\n",
        "<li>Load</li>\n",
        "<li>Oefening</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFnYZltvqLgt"
      },
      "source": [
        "## Big Data & PySpark\n",
        "We ontdekten wat Big Data is en welke uitdagingen het met zich mee brengt. Nu gaan we specifiek zien hoe je de ETL kan doen met PySpark. Om de scope van deze sessie te beperken focussen we enkel op de interne werking van PySpark. We gebruiken met andere woorden geen gedistribueerde file systemen, geen data orchestration noch gebruiken we een volwaardig data warehouse.\n",
        "\n",
        "><li>We zien hoe we een PySpark sessie opzetten.</li>\n",
        "><li>We ontdekken de PySpark functions voor data extraction. (Extract)</li>\n",
        "><li>We ontdekken de PySpark functions voor data transformation. (Transform)</li>\n",
        "><li>We maken gebruik van onze SQL kennis om dataframes te manipuleren.</li>\n",
        "><li>We ontleden een code snippet om de getransformeerde data door te sturen naar een Oracle DB (Load).</li>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N5-lspH_N8B"
      },
      "source": [
        "## Jupyter Basics\n",
        "Voer de code cells uit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ul54hAYyHyd"
      },
      "source": [
        "### Code cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j38beRUTCI5c"
      },
      "outputs": [],
      "source": [
        "# resultaten worden afgedrukt in jupyter\n",
        "2*3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jewe_e9CIYa"
      },
      "outputs": [],
      "source": [
        "# Importeren van externe libraries in python\n",
        "from math import pi\n",
        "print(pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8Y7w6_CCIIT"
      },
      "outputs": [],
      "source": [
        "print(\"Hello world\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOqLNkRKyUIS"
      },
      "source": [
        "### Text cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfbaUe-oq7DK"
      },
      "source": [
        "In jupyter kan je een onderscheid maken tussen code en tekst cells om je document structuur te geven."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6zdrH15_CCW"
      },
      "source": [
        "### Access to the shell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdO9sjSdEVnr"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF9e3lDDEX3I"
      },
      "outputs": [],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd6t0uFzuR4X"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6apGVff5h4ca"
      },
      "source": [
        "Installeer Dependencies: (Python is reeds geïnstalleerd in Google Colab)\n",
        "\n",
        "1.   Java 8\n",
        "2.   Apache Spark with hadoop and\n",
        "3.   Findspark (used to locate the spark in the system)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt7ZS1_wGgjn"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-17-jdk-headless -qq > /dev/null\n",
        "# Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-4.1.0/spark-4.1.0-bin-hadoop3.tgz\n",
        "!tar xf spark-4.1.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3x0ZRLxjMVr"
      },
      "source": [
        "*Set* Environment Variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdOOq4twHN1K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-4.1.0-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ACYMwhgHTYz"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR1zLBk1998Z"
      },
      "outputs": [],
      "source": [
        "# Importeer de findspark module\n",
        "import findspark\n",
        "# Initialiseer de Spark environment met behulp van findspark\n",
        "findspark.init()\n",
        "\n",
        "# Importeer een SparkSession module van PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "# Maak een SparkSession object genaamd 'spark'\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Stel Spark configuratie in om eager evaluation in te schakelen in de SQL REPL (Read-Eval-Print Loop)\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "\n",
        "# Print het 'spark' object, die de Spark sessie representeert\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QwZtWxZRCBn"
      },
      "source": [
        "## PySpark Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmIqq6xPK7m7"
      },
      "source": [
        "### Extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZwsr57lwPgq"
      },
      "source": [
        "#### Data inlezen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ3zmGACLKlN"
      },
      "outputs": [],
      "source": [
        "# Downloading and preprocessing Cars Data downloaded origianlly from https://perso.telecom-paristech.fr/eagan/class/igr204/datasets\n",
        "!wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpq2jYvIMOJy"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz6ALr5mMqZt"
      },
      "outputs": [],
      "source": [
        "# De spark.read functie extract data van een source file.\n",
        "# De read functie kan van verschillende sources lezen: csv, xml, text, json, db ...\n",
        "# header=True geeft aan dat de eerste rij een header is\n",
        "# sep=';' geeft het seperator character door om de kolommen te splitsen\n",
        "df = spark.read.csv('cars.csv', header=True, sep=\";\")\n",
        "df.show(5)\n",
        "df[\"Car\"].count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0lCS2LNwnoy"
      },
      "source": [
        "De bovenstaande cell laad de data in met spark.read in een Spark DataFrame. Een DataFrame is een 2D datastructuur waarin elke kolom een eigen datatype kan toegewezen krijgen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHXVmzOm2eHD"
      },
      "source": [
        "#### Data Inspecteren"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50LZ3S8_PMg_"
      },
      "source": [
        "Er zijn meerdere methodes om een dataframe(DF) in PySpark te zien:\n",
        "\n",
        "1.   `df.take(5)`  geeft een list van vijf Row objects terug.\n",
        "2.   `df.collect()` geeft ALLE data terug van het hele dataframe. Wees heel voorzichtig met het uitvoeren van deze methode, aangezien het makkelijk de driver node kan crashen.\n",
        "3.   `df.show()` is de meest gebruikte methode om data te tonen. Je kan enkele parameters meegeven, zoals de hoeveelheid van rijen en of er truncation mag gebruikt worden. Bijvoorbeeld: df.show(5, False) of df.show(5, truncate=False)\n",
        "4.   `df.limit(5)` zal een nieuw DataFrame terug geven door de eerste n rijen terug te geven. Aangezien de data gedistribueerd kan zijn, is er geen garantie dat deze methode elke keer dezelfde data teruggeeft.\n",
        "5.   `df.columns`  is de eigenschap die de verzameling van kolommen terug geeft."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1qqkqcfxM0v"
      },
      "outputs": [],
      "source": [
        "df.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9zwzswIxXF9"
      },
      "outputs": [],
      "source": [
        "df.limit(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o7jsazcu-13"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lfS2DhHuhPl"
      },
      "source": [
        "#### Dataframe Schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xX7hRoW_cXY"
      },
      "source": [
        "Er zijn twee methodes om datatypes van het schema (van een DataFrame) op te vragen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6qwTjGsNxrw"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCGTFlCWRPw4"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXx5ATpZ9oor"
      },
      "source": [
        "#### Schema Impliciet afleiden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TeflTUp8l29"
      },
      "source": [
        "Met de `inferschema=true` parameter kunnen we afdwingen dat de datatypes moeten afgeleid worden op basis van de data die uitgelezen wordt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qym5MjCi894N"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv('cars.csv', header=True, sep=\";\", inferSchema=True)\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6jTedYd-Dhb"
      },
      "source": [
        "We zien nu dat de gegevens correct zijn afgeleid, maar dat is niet altijd het geval. Soms wil je deze zelf instellen (wat ook mogelijk is). Spark heeft in deze dataset rekening gehouden met de precisie van kommagetallen en maakt van de Weight kolom een decimal(4,0). Wanneer je echter meerdere source bestanden ontvangt om te verwerken van verschillende klanten, dan kan je in problemen komen. *Niet elke klant slaat zijn data even accurraat op.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTVjYqeRuxWn"
      },
      "source": [
        "#### Expliciet Schema aanmaken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpsaQ4JMRUiS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ik62VX34SlFh"
      },
      "outputs": [],
      "source": [
        "# Maak de list van tupels van het schema in het formaat (column_name, data_type)\n",
        "labels = [\n",
        "     ('Car',StringType()),\n",
        "     ('MPG',DoubleType()),\n",
        "     ('Cylinders',IntegerType()),\n",
        "     ('Displacement',DoubleType()),\n",
        "     ('Horsepower',DoubleType()),\n",
        "     ('Weight',DoubleType()),\n",
        "     ('Acceleration',DoubleType()),\n",
        "     ('Model',IntegerType()),\n",
        "     ('Origin',StringType())\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-Fp5y_oU9SF"
      },
      "outputs": [],
      "source": [
        "# Maak het schema mbv een StructType op basis van de list\n",
        "schema = StructType([StructField (x[0], x[1], True) for x in labels])\n",
        "schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgC7gtL5VTls"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv('cars.csv', header=True, sep=\";\", schema=schema)\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn2EAhesVmx0"
      },
      "outputs": [],
      "source": [
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDCO3TEe95OY"
      },
      "source": [
        "We zien hier dat de data succesvol is geladen met de gegeven datatypes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsD48rckdHPe"
      },
      "source": [
        "### Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMlxdWfSY8ks"
      },
      "source": [
        "In de **Transform** sectie gaan we de volgende concepten overlopen in PySpark.\n",
        "\n",
        "<ol>\n",
        "  <li>Select:</li>\n",
        "  <ol>\n",
        "    <li>Selecting Columns</li>\n",
        "    <li>Selecting Multiple Columns</li>\n",
        "  </ol>\n",
        "  <li>Inserting Columns</li>\n",
        "  <ol>\n",
        "    <li>Kolommen toevoegen</li>\n",
        "    <li>Kolommen hernoemen</li>\n",
        "  </ol>\n",
        "  <li>Group by</li>\n",
        "  <li>Dropping Columns</li>\n",
        "  <li>DataFrame Operations on Rows</li>\n",
        "  <ol>\n",
        "    <li>Filtering</li>\n",
        "    <li>Distinct</li>\n",
        "    <li>Sort</li>\n",
        "    <li>Union</li>\n",
        "  </ol>\n",
        "</ol>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikGR5pDICTu7"
      },
      "source": [
        "#### Selecting Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VMwIwi2rj_o"
      },
      "source": [
        "Er zijn meerdere manieren om een kolom te selecteren. De punt-notatie lijkt misschien eenvoudiger, maar brengt problemen met zich mee. **Het is mogelijk dat je een kolomnaam hebt die overlapt met een eigenschap of functie van de DataFrame class**. Hierdoor zal je punt-notatie niet werken in deze gevallen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge9-_ygideWk"
      },
      "outputs": [],
      "source": [
        "# 1ste Optie\n",
        "# Kolomnamen zijn case sensitive met de punt-notatie\n",
        "print(df.Car)\n",
        "print(\"*\"*20)\n",
        "df.select(df.Car).show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md5zaET8dsr4"
      },
      "outputs": [],
      "source": [
        "# 2e Optie\n",
        "# Kolomnamen zijn niet case sensitive met de index-notatie\n",
        "print(df['car'])\n",
        "print(\"*\"*20)\n",
        "df.select(df['car']).show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gkf14sHec9a"
      },
      "outputs": [],
      "source": [
        "# 3e Optie\n",
        "# Kolomnamen zijn niet case sensitive met de `col()` functie\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col('car')).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6QsMfnNt3qF"
      },
      "source": [
        "#### Selecting Multiple Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp88S_Ug8Zoo"
      },
      "source": [
        "Wanneer je meerdere kolommen wil selecteren, dan geef je de gewenste kolom identifiers door aan de select methode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPjLMhZ6uAQR"
      },
      "outputs": [],
      "source": [
        "# 1ste methode\n",
        "print(df.Car, df.Cylinders)\n",
        "print(\"*\"*40)\n",
        "df.select(df.Car, df.Cylinders).show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMRMUrv7uHWa"
      },
      "outputs": [],
      "source": [
        "# 2e methode\n",
        "print(df['car'],df['cylinders'])\n",
        "print(\"*\"*40)\n",
        "df.select(df['car'],df['cylinders']).show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgQ20-4GugjR"
      },
      "outputs": [],
      "source": [
        "# 3e methode\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col('car'),col('cylinders')).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk98QhWhCTrY"
      },
      "source": [
        "\n",
        "<img src=\"https://i.ibb.co/M6cpLKk/oefeningen.png\" height=\"200\"/>\n",
        "\n",
        "**Probeer nu zelf in het df drie kolommen te selecteren die niet \"Car\" of \"Cylinders\" zijn. Zoek met behulp van je kennis over het schema de andere bestaande kolommen op en selecteer ze.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwSuU4MvCo2X"
      },
      "outputs": [],
      "source": [
        "# Hier volgt jouw code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85Lv3zSXCcOY"
      },
      "source": [
        "#### Kolommen toevoegen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Y7dcAHu-Uz"
      },
      "source": [
        "We overlopen de volgende drie scenario's\n",
        "\n",
        "1.   Kolom toevoegen\n",
        "2.   Meerdere kolommen toevoegen\n",
        "3.   Een nieuwe kolom aanmaken op basis van een bestaande kolom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFHUmRKZeCEV"
      },
      "outputs": [],
      "source": [
        "# 1. Kolom toevoegen\n",
        "# We voegen achteraan in het dataframe een nieuwe kolom toe\n",
        "from pyspark.sql.functions import lit\n",
        "df = df.withColumn('first_column',lit(1))\n",
        "# lit betekent literal. It vult de rij met de literal value.\n",
        "# Wanneer men statische data toevoegd, dan is het best practice om lit() te gebruiken.\n",
        "df.show(5,truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9772_mHwAqL"
      },
      "outputs": [],
      "source": [
        "# 2. Meerdere kolommen toevoegen\n",
        "# We voegen de kolommen second_column en third_column toe achteraan in het df\n",
        "df = df.withColumn('second_column', lit(2)) \\\n",
        "       .withColumn('third_column', lit('Third Column'))\n",
        "# lit betekent literal. It vult de rij met de literal value.\n",
        "# Wanneer men statische data toevoegd, dan is het best practice om lit() te gebruiken.\n",
        "df.show(5,truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGaQS_pOwx_b"
      },
      "outputs": [],
      "source": [
        "# 3.  Een nieuwe kolom aanmaken op basis van een bestaande kolom\n",
        "# We voegen een nieuwe kolom toe 'car_model' die de waarde heeft van car en model aan elkaar geconcateneerd met een spatie tussen\n",
        "from pyspark.sql.functions import concat\n",
        "df = df.withColumn('car_model', concat(df['Car'], lit(\" \"), df['model']))\n",
        "# lit betekent literal. It vult de rij met de literal value.\n",
        "# Wanneer men statische data toevoegd, dan is het best practice om lit() te gebruiken.\n",
        "df.show(5,truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeHExg-zxf5r"
      },
      "source": [
        "We gebruiken hier de `concat` methode om de twee kolommen aan elkaar vast te plakken.\n",
        "\n",
        "<img src=\"https://i.ibb.co/M6cpLKk/oefeningen.png\" height=\"200\"/>\n",
        "\n",
        "**Probeer nu zelf een kolom toe te voegen die het gewicht van de kolom Weight omzet van pounds (lb.) naar kilogram (kg). Noem de kolom Weight_kg**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftQlytdXCHRR"
      },
      "outputs": [],
      "source": [
        "# Hier volgt jouw code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlMf04i2CjDC"
      },
      "source": [
        "#### Kolommen hernoemen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwGKbSHvxxxG"
      },
      "source": [
        "We kunnen koklommen een nieuwe naam geven door middel van de  `withColumnRenamed` functie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJqgy6lKfk2o"
      },
      "outputs": [],
      "source": [
        "#Renaming a column in PySpark\n",
        "df = df.withColumnRenamed('first_column', 'new_column_one') \\\n",
        "       .withColumnRenamed('second_column', 'new_column_two') \\\n",
        "       .withColumnRenamed('third_column', 'new_column_three')\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CDifVC2Cnml"
      },
      "source": [
        "#### Group By"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wlB76FdyS0W"
      },
      "source": [
        "We kunnen allemaal al kolommen groeperen in SQL, maar wat is de syntax voor groeperingen in DataFrames?\n",
        "\n",
        "1.   Group By één kolom\n",
        "2.   Group By meerdere kolommen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1ek2opVfqea"
      },
      "outputs": [],
      "source": [
        "# Group By in PySpark\n",
        "df.groupBy('Origin').count().show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUh_TWcOysoL"
      },
      "outputs": [],
      "source": [
        "# Group By met meerdere kolommen in PySpark\n",
        "df.groupBy('Origin', 'Model').count().show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbpEj9fECrW3"
      },
      "source": [
        "#### Dropping Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsb9PXxpfnmh"
      },
      "outputs": [],
      "source": [
        "#Drop column in PySpark\n",
        "df = df.drop('new_column_one')\n",
        "df.show(5,truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKOXrXtvzK_0"
      },
      "outputs": [],
      "source": [
        "#Drop multiple columns:\n",
        "df = df.drop('new_column_two') \\\n",
        "       .drop('new_column_three')\n",
        "df.show(5,truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbKK5iHwmIoV"
      },
      "source": [
        "#### DataFrame Operations on Rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quwx3KlLzeq9"
      },
      "source": [
        "We zien de volgende onderwerpen voor rij-operaties:\n",
        "\n",
        "1.   Filtering\n",
        "2. \t Distinct\n",
        "3.   Sort\n",
        "4.   Union\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bKlvX-SH-Wy"
      },
      "source": [
        "##### Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNfcjOIknA3n"
      },
      "outputs": [],
      "source": [
        "# Filtering (Where) rows in PySpark\n",
        "total_count = df.count()\n",
        "print(\"TOTAL RECORD COUNT: \" + str(total_count))\n",
        "europe_filtered_count = df.filter(col('Origin')=='Europe').count()\n",
        "print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n",
        "df.filter(col('Origin')=='Europe').show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXJxRwBQ1lyd"
      },
      "outputs": [],
      "source": [
        "# Filtering op basis van meerdere voorwaardes\n",
        "total_count = df.count()\n",
        "print(\"TOTAL RECORD COUNT: \" + str(total_count))\n",
        "europe_filtered_count = df.filter((col('Origin')=='Europe') &\n",
        "                                  (col('Cylinders')==4)).count() # Tweede voorwaarde\n",
        "print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n",
        "df.filter(col('Origin')=='Europe').show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YC_XyVlsd5x"
      },
      "source": [
        "Nu weet je hoe je succesvol filters kan toepassen.\n",
        "\n",
        "<img src=\"https://i.ibb.co/M6cpLKk/oefeningen.png\" height=\"200\"/>\n",
        "\n",
        "**Probeer nu zelf een filter te schrijven voor de Weight_kg kolom die alle wagens toont die meer wegen dan 2000 kg en een Horsepower hebben boven 120. Tel het verschil in rijen ten opzicht van het oorspronkelijke dataframe.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfnY67dZsdWS"
      },
      "outputs": [],
      "source": [
        "# Hier volgt jouw code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLU-a4auIEvh"
      },
      "source": [
        "##### Distinct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1RKg1UrmBQz"
      },
      "outputs": [],
      "source": [
        "# Unieke Rijen opvragen in PySpark\n",
        "df.select('Origin').distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LQWXPXt0g0N"
      },
      "outputs": [],
      "source": [
        "# Unieke rijen opvragen op basis van meerdere voorwaardes\n",
        "df.select('Origin','model').distinct().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-069UYUwIIYI"
      },
      "source": [
        "##### Sort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZpeJvz0nkBI"
      },
      "outputs": [],
      "source": [
        "# Sorteren van rijen in PySpark\n",
        "# By default is de data gesorteerd in \"ascending\" order\n",
        "df.orderBy('Cylinders').show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1CEwofMJV-D"
      },
      "outputs": [],
      "source": [
        "# Indien je dit niet wilt, dan zal je de ascending parameter moeten gebruiken\n",
        "df.orderBy('Cylinders', ascending=False).show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx3W4aeL5A4O"
      },
      "outputs": [],
      "source": [
        "# Combineren van groupBy met orderBy\n",
        "df.groupBy(\"Origin\").count().orderBy('count', ascending=False).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN0-A_JsIX-X"
      },
      "source": [
        "##### Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH0KOaBrJt6v"
      },
      "source": [
        "We gaan twee verschillende methodes zien om een union uit te voeren tussen twee dataframes. Het is belangrijk om de verschillen te kennen tussen de methodes, zodat je de juiste kan kiezen.\n",
        "\n",
        "*   `union()` – Deze methode kan je gebruiken om twee dataframes te mergen met dezelfde structuur/schema. Als de twee dataframes niet hetzelfde zijn, dan krijg je een error.\n",
        "*   `unionByName()` - Deze methode wordt gebruikt om twee dataframes te mergen op basis van kolom naam. Het maakt niet uit in welke volgorde de kolommen in de dataframes staan, zolang de kolomnamen gelinkt kunnen worden.\n",
        "\n",
        "*In andere SQL talen, zorgen Unions ervoor dat duplicaten verwijdert worden, maar in PySpark gebeurt dit niet. Het is best practice om achteraf een distinct() filter of dropDuplicates() uit te voeren.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCZIzfYmnx--"
      },
      "outputs": [],
      "source": [
        "# 1. Union met de kolommen in gelijke volgorde\n",
        "df = spark.read.csv('cars.csv', header=True, sep=\";\", inferSchema=True)\n",
        "europe_cars = df.filter((col('Origin')=='Europe') & (col('Cylinders')==5))\n",
        "japan_cars = df.filter((col('Origin')=='Japan') & (col('Cylinders')==3))\n",
        "print(\"EUROPE CARS: \"+str(europe_cars.count()))\n",
        "print(\"JAPAN CARS: \"+str(japan_cars.count()))\n",
        "print(\"AFTER UNION: \"+str(europe_cars.union(japan_cars).count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pfPzVOFqC_8"
      },
      "source": [
        "Hier kan je zien hoe de data van twee verschillende dataframes aan elkaar gemerged worden. De Europese autos met 5 cylinders en Japanse autos met 3 cylinders staan nu samen in één dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjWjzWBoMxx0"
      },
      "outputs": [],
      "source": [
        "# 2. Union Met kolommen in een andere volgorde, maar met gelijke namen\n",
        "# Twee dataframes aanmaken met een andere volgorde van kolommen:\n",
        "df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
        "df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
        "df1.unionByName(df2).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHjILb1DriuX"
      },
      "source": [
        "### Data Manipulation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3vlC7ZerlKb"
      },
      "outputs": [],
      "source": [
        "# Beschikbare data manipulatie functies in PySpark\n",
        "from pyspark.sql import functions\n",
        "# Net zoals in python, kunnen we de dir functie gebruiken om de beschikbare functies op te roepen van de library\n",
        "print(dir(functions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIKigra7A34e"
      },
      "source": [
        "#### String Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63QDccSjBqC4"
      },
      "outputs": [],
      "source": [
        "# Laad de data opnieuw (een nieuwe start)\n",
        "from pyspark.sql.functions import col\n",
        "df = spark.read.csv('cars.csv', header=True, sep=\";\", inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiXWN8DUA9x6"
      },
      "source": [
        "In onderstaande code cell worden de lower, upper en substring functies gebruikt om de naam van de auto te manipuleren. De alias methode wordt gebruikt om de kolom een nieuwe naam te geven."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52Gh9c99BZFr"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col,lower, upper, substring\n",
        "# Met help kan je de details van de functie afdrukken\n",
        "help(substring)\n",
        "# alias is used to rename the column name in the output\n",
        "df.select(col('Car'),lower(col('Car')),upper(col('Car')),substring(col('Car'),1,4).alias(\"concatenated value\")).show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldtA0wk9BMkT"
      },
      "source": [
        "#### Numerieke functies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmz4G5LVBOs6"
      },
      "source": [
        "**Toon de oudste datum en de meest recente datum.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBDDH-YpBbdk"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import min, max\n",
        "df.select(min(col('Weight')), max(col('Weight'))).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTg-Royz7Nvi"
      },
      "source": [
        "**Voeg tien toe aan het minimum en maximum gewicht.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeiemMsI7Vm2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import min, max, lit\n",
        "df.select(min(col('Weight'))+lit(10), max(col('Weight')+lit(10))).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OZElEvcGOD1"
      },
      "source": [
        "### Joins in PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJBC7r3JFyCL"
      },
      "outputs": [],
      "source": [
        "# Maak twee dataframes\n",
        "cars_df = spark.createDataFrame([[1, 'Car A'],[2, 'Car B'],[3, 'Car C']], [\"id\", \"car_name\"])\n",
        "car_price_df = spark.createDataFrame([[1, 1000],[2, 2000],[3, 3000]], [\"id\", \"car_price\"])\n",
        "cars_df.show()\n",
        "car_price_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7Py4EYyKJTN"
      },
      "outputs": [],
      "source": [
        "# Voer een inner join uit, zodat we de id, naam en prijs van elke auto zien in een rij.\n",
        "cars_df.join(car_price_df, cars_df.id == car_price_df.id, 'inner').select(cars_df['id'],cars_df['car_name'],car_price_df['car_price']).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj0mPaHU5i5n"
      },
      "source": [
        "Zoals hierboven aangetoond kunnen we net zoals gewoonlijk inner joins uitvoeren tussen twee dataframes. De volgende joins zijn ook ondersteund in PySpark:\n",
        "1. inner (default)\n",
        "2. cross\n",
        "3. outer\n",
        "4. full\n",
        "5. full_outer\n",
        "6. left\n",
        "7. left_outer\n",
        "8. right\n",
        "9. right_outer\n",
        "10. left_semi\n",
        "11. left_anti"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNPhsx8P2tUH"
      },
      "source": [
        "## Spark SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHMvBBAh23cw"
      },
      "source": [
        "SQL bestaat sinds de jaren 1970. Er zijn bijzonder veel ontwikkelaars die er dagelijks in schrijven en er hun brood mee verdienen. Naarmate big data populairder werd, groeide het tekort aan professionals met de technische kennis in data engineering om dit tekort op te vangen heeft men Spark SQL gemaakt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2DaK9-D7QkX"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = spark.read.csv('cars.csv', header=True, sep=\";\")\n",
        "# Register Temporary Table\n",
        "df.createOrReplaceTempView(\"temp\")\n",
        "# Select all data from temp table\n",
        "sql_dataframe = spark.sql(\"select * from temp limit 5\")\n",
        "print(\"spark sql\")\n",
        "sql_dataframe.show(5)\n",
        "print(\"spark dataframe\")\n",
        "sql_dataframe.orderBy(col('Horsepower'), ascending=False).show(5)\n",
        "# Select count of data in table\n",
        "spark.sql(\"select count(*) as total_count from temp\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i32WE8j_ec8"
      },
      "source": [
        "Met `createOrReplaceTempView(\"temp_view_name\")` kunnen we een dataframe omzetten naar een sql view waarin we traditionele queries kunnen uitvoeren. Nadat we een query uitgevoerd hebben op de view kunnen we het resultaat opslaan in een SQL DataFrame. Dit is niet hetzelfde als een Spark Dataframe (ook is het geen pandas DataFrame).\n",
        "\n",
        "Om terug te werken met een DataFrame nadat je de data hebt opgeslagen als een view gebruik je de `spark.table(\"temp_view_name\")` methode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaF90Z5AU--9"
      },
      "source": [
        "# Load\n",
        "\n",
        "Tot slot is hier een voorbeeld script om data te \"load\"-en naar een Oracle database. Deze database bevat de verwerkte versie van je gegevens. Je slaat hier geen gegevens op, maar informatie.\n",
        "\n",
        "Men load de data na transformatie naar een data warehouse. Een data warehouse bevat data analyse tools en meestal een RDBMS.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Initieer de volgende waardes met jouw connection string\n",
        "driver = 'oracle.jdbc.driver.OracleDriver'\n",
        "url = 'jdbc:oracle:thin@localhost:1521/XEPDB1'\n",
        "user = 'studend'\n",
        "password = 'pxl'\n",
        "table = 'table_name'\n",
        "\n",
        "# Maak een nieuwe table met \"createTableOptions\"\n",
        "df.write.format('jdbc').option('driver', driver)\\\n",
        "  .option('url', url)\\\n",
        "  .option('user', user)\\\n",
        "  .option('password', password)\\\n",
        "  .mode('overwrite')\\\n",
        "  .option('createTableOptions', '')\\\n",
        "  .option('dbtable', table).save()\n",
        "\n",
        "# Append aan een bestaande table\n",
        "df.write.format('jdbc').option('driver', driver)\\\n",
        "  .option('url', url)\\\n",
        "  .option('user', user)\\\n",
        "  .option('password', password)\\\n",
        "  .mode('append')\\\n",
        "  .option('dbtable', table).save()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6sJk8BlXrX-"
      },
      "source": [
        "# Oefening:\n",
        "\n",
        "Inspecteer de volgende vier csv bestanden. Je werkt voor een bedrijfketen die over heel de wereld winkels heeft. Op het einde van elke werkdag sturen de winkels een verslag door (csv) waarin er een overzicht te vinden is van hoeveel producten er verkocht zijn aan welke prijs.\n",
        "\n",
        "Je produceert je producten zelf en weet wat de productiekost is van elk product. De verschillende winkels gebruiken echter hun regionale data (munteenheid). Bovendien zijn de winkels zelf vrij om hun eigen prijzen te kiezen. Na wisselkoers hanteren niet alle winkels dezelfde prijzen.\n",
        "\n",
        "**Fase 1: Extract**</br>\n",
        "Jij moet er voor zorgen dat elk van de csv's ingeladen kan worden (EXTRACT). Ontdek de data. Wat is het schema? Maak een dataframe voor elke csv.\n",
        "\n",
        "Je vindt de data terug via de volgende links:\n",
        "\n",
        "\n",
        "1. https://github.com/PXL-Labs/Data-Expert/blob/main/europe-sales.csv\n",
        "2. https://github.com/PXL-Labs/Data-Expert/blob/main/japan-sales.csv\n",
        "3. https://github.com/PXL-Labs/Data-Expert/blob/main/production-costs.csv\n",
        "4. https://github.com/PXL-Labs/Data-Expert/blob/main/us-sales.csv\n",
        "\n",
        "\n",
        "**Fase 2: Transform**</br>\n",
        "\n",
        "1.   Converteer elke munt naar euro (yen en dollar naar euro)\n",
        "2.   Voeg aan elk winkelrapport een kolom toe `ProductionCost` met de productiekosten van elk verkocht product.\n",
        "3.   Voeg een kolom `Profit` toe aan elk winkelrapport waarin de totale opbrengst berekend wordt. *(Je mag hier gewoon de verkoopsprijs ten opzicht van productiekost hanteren. Je moet niet rekening houden met transport kosten, onderhoud, werknemers uitbetalen, ...)*\n",
        "4.   Voeg een kolom `CountryCode` toe aan elk winkelrapport met een country code voor elke winkel (EUR, US en JP)\n",
        "5.   Verwijder de Region en Currency kolommen.\n",
        "6.   Voeg alle winkelrapporten samen in één groot dataframe.\n",
        "\n",
        "**Spark SQL**:\n",
        "7.   Registreer je temp view van het dataframe dat alle gegevens bevat van de drie csv bestanden.\n",
        "8.   Schrijf een query met spark.sql die berekent wat de totale opbrengst is.\n",
        "9.   Schrijf een query met spark.sql die berekent wat het meest winstgevend product was per winkel.\n",
        "\n",
        "<img src=\"https://i.ibb.co/M6cpLKk/oefeningen.png\" height=\"300\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TFAu35teE_5"
      },
      "outputs": [],
      "source": [
        "# Oefening 1:\n",
        "# download files\n",
        "!wget -O us_sales.csv https://github.com/PXL-Labs/Data-Expert/blob/main/us-sales.csv?raw=true\n",
        "!wget -O production_costs.csv https://github.com/PXL-Labs/Data-Expert/blob/main/production-costs.csv?raw=true\n",
        "!wget -O japan_sales.csv https://github.com/PXL-Labs/Data-Expert/blob/main/japan-sales.csv?raw=true\n",
        "!wget -O europe_sales.csv https://github.com/PXL-Labs/Data-Expert/blob/main/europe-sales.csv?raw=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKH5u4QWfRNL"
      },
      "source": [
        "Controleer de aanwezigheid van alle 4 bestanden:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLSdm2hYfL7Q"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4ndeXMAe50X"
      },
      "outputs": [],
      "source": [
        "# Oplossing"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QlMf04i2CjDC",
        "4CDifVC2Cnml",
        "CbpEj9fECrW3"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
